{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cVu3aj7nl4Tt",
        "outputId": "1fb70404-d71f-45a8-bd61-3e1009c28752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 357 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 50.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Installing collected packages: pandas, nlpaug\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed nlpaug-1.1.10 pandas-1.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.3\n",
            "  Downloading pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.3) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.3) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.3) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nlpaug 1.1.10 requires pandas>=1.2.0, but you have pandas 1.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install numpy requests nlpaug\n",
        "!pip install pandas==1.1.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDOJ43WYmGua",
        "outputId": "0a030d8e-5712-4f82-bf04-cc5c896a39bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bcDzP-8imH-3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nlpaug\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnLPZ0zZolXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e46dc8-6c24-4362-db91-5e7b256dc5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The length of dataset is: 79305\n"
          ]
        }
      ],
      "source": [
        "#function to split dataframe into chuncks\n",
        "def split_dataframe(df, chunk_size = 500):\n",
        "  chunks = list()\n",
        "  num_chunks = len(df) // chunk_size + 1\n",
        "  for i in range(num_chunks):\n",
        "      chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
        "  return chunks\n",
        "\n",
        "  #function reads data collected and divides into chunks for data preprocessing\n",
        "#this is a one time function call to divide into chunks\n",
        "def divide_chunks():\n",
        "\n",
        "  #read data \n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/EmbeddingFileVocabData/Final_data_Y_D.csv\")\n",
        "\n",
        "  df = df[df[\"Label\"]!=\"label\"]\n",
        "  df = df[df[\"Label\"]!=\"Label\"]\n",
        "\n",
        "  df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "  df[\"Label\"] = df[\"Label\"].apply(lambda x: int(x))\n",
        "\n",
        "  #perform augmentation for only hate speech values as they are less\n",
        "  df = df[df.Label.values==1]\n",
        "  \n",
        "  #rest index\n",
        "  df.reset_index(drop = True, inplace=True)\n",
        "  print(\"\\nThe length of dataset is: {}\".format(len(df)))\n",
        "\n",
        "\n",
        "  #split dataframe into chuncks\n",
        "  sp_df = split_dataframe(df)\n",
        "\n",
        "  #store the chuncks into a seprate csv file\n",
        "  for i in range(0,len(sp_df)):\n",
        "    sp_df[i].to_csv(\"/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/augx{}.csv\".format(i),index=False)\n",
        "\n",
        "#divide_chunks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K04vPUGHmZvA"
      },
      "outputs": [],
      "source": [
        "#this function is used to perfrom data augmentation so we have uniformity in data\n",
        "def aug_data(df):\n",
        "  \n",
        "  #perform a substitue augmentation\n",
        "  aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased', action=\"substitute\")\n",
        "\n",
        "  #perform a insertion augmentation\n",
        "  aug1 = naw.ContextualWordEmbsAug(\n",
        "      model_path='bert-base-uncased', action=\"insert\")\n",
        "\n",
        "  aug2 = naw.SynonymAug(aug_src='wordnet', lang='eng')\n",
        "\n",
        "  #perform augmentation for only hate speech values as they are less\n",
        "  data = df[df.Label.values==1]\n",
        "\n",
        "  aug_sentence = []\n",
        "  lr=iter([item for item in range(0,len(df))])\n",
        "  for value in data[\"Content\"]:\n",
        "    print(\"Processing for text {}\".format(next(lr)))\n",
        "    a1 = aug.augment(value,2) \n",
        "    for i in a1:\n",
        "      aug_sentence.append(i)\n",
        "    aug_sentence.append(aug1.augment(value,1))\n",
        "    aug_sentence.append(aug2.augment(value))\n",
        "\n",
        "  new_df = pd.DataFrame (aug_sentence, columns = ['Content'])\n",
        "  new_df[\"Label\"]=1\n",
        "\n",
        "  #save augmented data \n",
        "  new_df.to_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data_new.csv',mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUzHEDy9mg0p"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  #get the final cleaned data\n",
        "  df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/augx155.csv')\n",
        "  print(\"\\nThe length of dataset is: {}\".format(len(df)))\n",
        "\n",
        "  aug_data(df)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PT2lFrWooXl",
        "outputId": "cfcbe140-eaa2-497c-a7a4-b1ee2754852f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The length of dataset is: 500\n",
            "Processing for text 0\n",
            "Processing for text 1\n",
            "Processing for text 2\n",
            "Processing for text 3\n",
            "Processing for text 4\n",
            "Processing for text 5\n",
            "Processing for text 6\n",
            "Processing for text 7\n",
            "Processing for text 8\n",
            "Processing for text 9\n",
            "Processing for text 10\n",
            "Processing for text 11\n",
            "Processing for text 12\n",
            "Processing for text 13\n",
            "Processing for text 14\n",
            "Processing for text 15\n",
            "Processing for text 16\n",
            "Processing for text 17\n",
            "Processing for text 18\n",
            "Processing for text 19\n",
            "Processing for text 20\n",
            "Processing for text 21\n",
            "Processing for text 22\n",
            "Processing for text 23\n",
            "Processing for text 24\n",
            "Processing for text 25\n",
            "Processing for text 26\n",
            "Processing for text 27\n",
            "Processing for text 28\n",
            "Processing for text 29\n",
            "Processing for text 30\n",
            "Processing for text 31\n",
            "Processing for text 32\n",
            "Processing for text 33\n",
            "Processing for text 34\n",
            "Processing for text 35\n",
            "Processing for text 36\n",
            "Processing for text 37\n",
            "Processing for text 38\n",
            "Processing for text 39\n",
            "Processing for text 40\n",
            "Processing for text 41\n",
            "Processing for text 42\n",
            "Processing for text 43\n",
            "Processing for text 44\n",
            "Processing for text 45\n",
            "Processing for text 46\n",
            "Processing for text 47\n",
            "Processing for text 48\n",
            "Processing for text 49\n",
            "Processing for text 50\n",
            "Processing for text 51\n",
            "Processing for text 52\n",
            "Processing for text 53\n",
            "Processing for text 54\n",
            "Processing for text 55\n",
            "Processing for text 56\n",
            "Processing for text 57\n",
            "Processing for text 58\n",
            "Processing for text 59\n",
            "Processing for text 60\n",
            "Processing for text 61\n",
            "Processing for text 62\n",
            "Processing for text 63\n",
            "Processing for text 64\n",
            "Processing for text 65\n",
            "Processing for text 66\n",
            "Processing for text 67\n",
            "Processing for text 68\n",
            "Processing for text 69\n",
            "Processing for text 70\n",
            "Processing for text 71\n",
            "Processing for text 72\n",
            "Processing for text 73\n",
            "Processing for text 74\n",
            "Processing for text 75\n",
            "Processing for text 76\n",
            "Processing for text 77\n",
            "Processing for text 78\n",
            "Processing for text 79\n",
            "Processing for text 80\n",
            "Processing for text 81\n",
            "Processing for text 82\n",
            "Processing for text 83\n",
            "Processing for text 84\n",
            "Processing for text 85\n",
            "Processing for text 86\n",
            "Processing for text 87\n",
            "Processing for text 88\n",
            "Processing for text 89\n",
            "Processing for text 90\n",
            "Processing for text 91\n",
            "Processing for text 92\n",
            "Processing for text 93\n",
            "Processing for text 94\n",
            "Processing for text 95\n",
            "Processing for text 96\n",
            "Processing for text 97\n",
            "Processing for text 98\n",
            "Processing for text 99\n",
            "Processing for text 100\n",
            "Processing for text 101\n",
            "Processing for text 102\n",
            "Processing for text 103\n",
            "Processing for text 104\n",
            "Processing for text 105\n",
            "Processing for text 106\n",
            "Processing for text 107\n",
            "Processing for text 108\n",
            "Processing for text 109\n",
            "Processing for text 110\n",
            "Processing for text 111\n",
            "Processing for text 112\n",
            "Processing for text 113\n",
            "Processing for text 114\n",
            "Processing for text 115\n",
            "Processing for text 116\n",
            "Processing for text 117\n",
            "Processing for text 118\n",
            "Processing for text 119\n",
            "Processing for text 120\n",
            "Processing for text 121\n",
            "Processing for text 122\n",
            "Processing for text 123\n",
            "Processing for text 124\n",
            "Processing for text 125\n",
            "Processing for text 126\n",
            "Processing for text 127\n",
            "Processing for text 128\n",
            "Processing for text 129\n",
            "Processing for text 130\n",
            "Processing for text 131\n",
            "Processing for text 132\n",
            "Processing for text 133\n",
            "Processing for text 134\n",
            "Processing for text 135\n",
            "Processing for text 136\n",
            "Processing for text 137\n",
            "Processing for text 138\n",
            "Processing for text 139\n",
            "Processing for text 140\n",
            "Processing for text 141\n",
            "Processing for text 142\n",
            "Processing for text 143\n",
            "Processing for text 144\n",
            "Processing for text 145\n",
            "Processing for text 146\n",
            "Processing for text 147\n",
            "Processing for text 148\n",
            "Processing for text 149\n",
            "Processing for text 150\n",
            "Processing for text 151\n",
            "Processing for text 152\n",
            "Processing for text 153\n",
            "Processing for text 154\n",
            "Processing for text 155\n",
            "Processing for text 156\n",
            "Processing for text 157\n",
            "Processing for text 158\n",
            "Processing for text 159\n",
            "Processing for text 160\n",
            "Processing for text 161\n",
            "Processing for text 162\n",
            "Processing for text 163\n",
            "Processing for text 164\n",
            "Processing for text 165\n",
            "Processing for text 166\n",
            "Processing for text 167\n",
            "Processing for text 168\n",
            "Processing for text 169\n",
            "Processing for text 170\n",
            "Processing for text 171\n",
            "Processing for text 172\n",
            "Processing for text 173\n",
            "Processing for text 174\n",
            "Processing for text 175\n",
            "Processing for text 176\n",
            "Processing for text 177\n",
            "Processing for text 178\n",
            "Processing for text 179\n",
            "Processing for text 180\n",
            "Processing for text 181\n",
            "Processing for text 182\n",
            "Processing for text 183\n",
            "Processing for text 184\n",
            "Processing for text 185\n",
            "Processing for text 186\n",
            "Processing for text 187\n",
            "Processing for text 188\n",
            "Processing for text 189\n",
            "Processing for text 190\n",
            "Processing for text 191\n",
            "Processing for text 192\n",
            "Processing for text 193\n",
            "Processing for text 194\n",
            "Processing for text 195\n",
            "Processing for text 196\n",
            "Processing for text 197\n",
            "Processing for text 198\n",
            "Processing for text 199\n",
            "Processing for text 200\n",
            "Processing for text 201\n",
            "Processing for text 202\n",
            "Processing for text 203\n",
            "Processing for text 204\n",
            "Processing for text 205\n",
            "Processing for text 206\n",
            "Processing for text 207\n",
            "Processing for text 208\n",
            "Processing for text 209\n",
            "Processing for text 210\n",
            "Processing for text 211\n",
            "Processing for text 212\n",
            "Processing for text 213\n",
            "Processing for text 214\n",
            "Processing for text 215\n",
            "Processing for text 216\n",
            "Processing for text 217\n",
            "Processing for text 218\n",
            "Processing for text 219\n",
            "Processing for text 220\n",
            "Processing for text 221\n",
            "Processing for text 222\n",
            "Processing for text 223\n",
            "Processing for text 224\n",
            "Processing for text 225\n",
            "Processing for text 226\n",
            "Processing for text 227\n",
            "Processing for text 228\n",
            "Processing for text 229\n",
            "Processing for text 230\n",
            "Processing for text 231\n",
            "Processing for text 232\n",
            "Processing for text 233\n",
            "Processing for text 234\n",
            "Processing for text 235\n",
            "Processing for text 236\n",
            "Processing for text 237\n",
            "Processing for text 238\n",
            "Processing for text 239\n",
            "Processing for text 240\n",
            "Processing for text 241\n",
            "Processing for text 242\n",
            "Processing for text 243\n",
            "Processing for text 244\n",
            "Processing for text 245\n",
            "Processing for text 246\n",
            "Processing for text 247\n",
            "Processing for text 248\n",
            "Processing for text 249\n",
            "Processing for text 250\n",
            "Processing for text 251\n",
            "Processing for text 252\n",
            "Processing for text 253\n",
            "Processing for text 254\n",
            "Processing for text 255\n",
            "Processing for text 256\n",
            "Processing for text 257\n",
            "Processing for text 258\n",
            "Processing for text 259\n",
            "Processing for text 260\n",
            "Processing for text 261\n",
            "Processing for text 262\n",
            "Processing for text 263\n",
            "Processing for text 264\n",
            "Processing for text 265\n",
            "Processing for text 266\n",
            "Processing for text 267\n",
            "Processing for text 268\n",
            "Processing for text 269\n",
            "Processing for text 270\n",
            "Processing for text 271\n",
            "Processing for text 272\n",
            "Processing for text 273\n",
            "Processing for text 274\n",
            "Processing for text 275\n",
            "Processing for text 276\n",
            "Processing for text 277\n",
            "Processing for text 278\n",
            "Processing for text 279\n",
            "Processing for text 280\n",
            "Processing for text 281\n",
            "Processing for text 282\n",
            "Processing for text 283\n",
            "Processing for text 284\n",
            "Processing for text 285\n",
            "Processing for text 286\n",
            "Processing for text 287\n",
            "Processing for text 288\n",
            "Processing for text 289\n",
            "Processing for text 290\n",
            "Processing for text 291\n",
            "Processing for text 292\n",
            "Processing for text 293\n",
            "Processing for text 294\n",
            "Processing for text 295\n",
            "Processing for text 296\n",
            "Processing for text 297\n",
            "Processing for text 298\n",
            "Processing for text 299\n",
            "Processing for text 300\n",
            "Processing for text 301\n",
            "Processing for text 302\n",
            "Processing for text 303\n",
            "Processing for text 304\n",
            "Processing for text 305\n",
            "Processing for text 306\n",
            "Processing for text 307\n",
            "Processing for text 308\n",
            "Processing for text 309\n",
            "Processing for text 310\n",
            "Processing for text 311\n",
            "Processing for text 312\n",
            "Processing for text 313\n",
            "Processing for text 314\n",
            "Processing for text 315\n",
            "Processing for text 316\n",
            "Processing for text 317\n",
            "Processing for text 318\n",
            "Processing for text 319\n",
            "Processing for text 320\n",
            "Processing for text 321\n",
            "Processing for text 322\n",
            "Processing for text 323\n",
            "Processing for text 324\n",
            "Processing for text 325\n",
            "Processing for text 326\n",
            "Processing for text 327\n",
            "Processing for text 328\n",
            "Processing for text 329\n",
            "Processing for text 330\n",
            "Processing for text 331\n",
            "Processing for text 332\n",
            "Processing for text 333\n",
            "Processing for text 334\n",
            "Processing for text 335\n",
            "Processing for text 336\n",
            "Processing for text 337\n",
            "Processing for text 338\n",
            "Processing for text 339\n",
            "Processing for text 340\n",
            "Processing for text 341\n",
            "Processing for text 342\n",
            "Processing for text 343\n",
            "Processing for text 344\n",
            "Processing for text 345\n",
            "Processing for text 346\n",
            "Processing for text 347\n",
            "Processing for text 348\n",
            "Processing for text 349\n",
            "Processing for text 350\n",
            "Processing for text 351\n",
            "Processing for text 352\n",
            "Processing for text 353\n",
            "Processing for text 354\n",
            "Processing for text 355\n",
            "Processing for text 356\n",
            "Processing for text 357\n",
            "Processing for text 358\n",
            "Processing for text 359\n",
            "Processing for text 360\n",
            "Processing for text 361\n",
            "Processing for text 362\n",
            "Processing for text 363\n",
            "Processing for text 364\n",
            "Processing for text 365\n",
            "Processing for text 366\n",
            "Processing for text 367\n",
            "Processing for text 368\n",
            "Processing for text 369\n",
            "Processing for text 370\n",
            "Processing for text 371\n",
            "Processing for text 372\n",
            "Processing for text 373\n",
            "Processing for text 374\n",
            "Processing for text 375\n",
            "Processing for text 376\n",
            "Processing for text 377\n",
            "Processing for text 378\n",
            "Processing for text 379\n",
            "Processing for text 380\n",
            "Processing for text 381\n",
            "Processing for text 382\n",
            "Processing for text 383\n",
            "Processing for text 384\n",
            "Processing for text 385\n",
            "Processing for text 386\n",
            "Processing for text 387\n",
            "Processing for text 388\n",
            "Processing for text 389\n",
            "Processing for text 390\n",
            "Processing for text 391\n",
            "Processing for text 392\n",
            "Processing for text 393\n",
            "Processing for text 394\n",
            "Processing for text 395\n",
            "Processing for text 396\n",
            "Processing for text 397\n",
            "Processing for text 398\n",
            "Processing for text 399\n",
            "Processing for text 400\n",
            "Processing for text 401\n",
            "Processing for text 402\n",
            "Processing for text 403\n",
            "Processing for text 404\n",
            "Processing for text 405\n",
            "Processing for text 406\n",
            "Processing for text 407\n",
            "Processing for text 408\n",
            "Processing for text 409\n",
            "Processing for text 410\n",
            "Processing for text 411\n",
            "Processing for text 412\n",
            "Processing for text 413\n",
            "Processing for text 414\n",
            "Processing for text 415\n",
            "Processing for text 416\n",
            "Processing for text 417\n",
            "Processing for text 418\n",
            "Processing for text 419\n",
            "Processing for text 420\n",
            "Processing for text 421\n",
            "Processing for text 422\n",
            "Processing for text 423\n",
            "Processing for text 424\n",
            "Processing for text 425\n",
            "Processing for text 426\n",
            "Processing for text 427\n",
            "Processing for text 428\n",
            "Processing for text 429\n",
            "Processing for text 430\n",
            "Processing for text 431\n",
            "Processing for text 432\n",
            "Processing for text 433\n",
            "Processing for text 434\n",
            "Processing for text 435\n",
            "Processing for text 436\n",
            "Processing for text 437\n",
            "Processing for text 438\n",
            "Processing for text 439\n",
            "Processing for text 440\n",
            "Processing for text 441\n",
            "Processing for text 442\n",
            "Processing for text 443\n",
            "Processing for text 444\n",
            "Processing for text 445\n",
            "Processing for text 446\n",
            "Processing for text 447\n",
            "Processing for text 448\n",
            "Processing for text 449\n",
            "Processing for text 450\n",
            "Processing for text 451\n",
            "Processing for text 452\n",
            "Processing for text 453\n",
            "Processing for text 454\n",
            "Processing for text 455\n",
            "Processing for text 456\n",
            "Processing for text 457\n",
            "Processing for text 458\n",
            "Processing for text 459\n",
            "Processing for text 460\n",
            "Processing for text 461\n",
            "Processing for text 462\n",
            "Processing for text 463\n",
            "Processing for text 464\n",
            "Processing for text 465\n",
            "Processing for text 466\n",
            "Processing for text 467\n",
            "Processing for text 468\n",
            "Processing for text 469\n",
            "Processing for text 470\n",
            "Processing for text 471\n",
            "Processing for text 472\n",
            "Processing for text 473\n",
            "Processing for text 474\n",
            "Processing for text 475\n",
            "Processing for text 476\n",
            "Processing for text 477\n",
            "Processing for text 478\n",
            "Processing for text 479\n",
            "Processing for text 480\n",
            "Processing for text 481\n",
            "Processing for text 482\n",
            "Processing for text 483\n",
            "Processing for text 484\n",
            "Processing for text 485\n",
            "Processing for text 486\n",
            "Processing for text 487\n",
            "Processing for text 488\n",
            "Processing for text 489\n",
            "Processing for text 490\n",
            "Processing for text 491\n",
            "Processing for text 492\n",
            "Processing for text 493\n",
            "Processing for text 494\n",
            "Processing for text 495\n",
            "Processing for text 496\n",
            "Processing for text 497\n",
            "Processing for text 498\n",
            "Processing for text 499\n"
          ]
        }
      ],
      "source": [
        "if __name__:main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#additional process applied on final data\n",
        "def process(df,text):\n",
        "  \n",
        "  df = df[df[\"Label\"]!=\"label\"]\n",
        "  df = df[df[\"Label\"]!=\"Label\"]\n",
        "\n",
        "  df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "  df[\"Label\"] = df[\"Label\"].apply(lambda x: int(x))\n",
        "\n",
        "  #rest index\n",
        "  df.reset_index(drop = True, inplace=True)\n",
        "  print(\"\\nThe length of dataset is: {}\".format(len(df)))\n",
        "\n",
        "  print(\"\\nTotal number of values for classes is: {}\".format(df[\"Label\"].value_counts()))\n",
        "\n",
        "  #write data to the path\n",
        "  df.to_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/FinalDataAndModel/{}.csv'.format(text),index=False)\n"
      ],
      "metadata": {
        "id": "ldHtQOVHUYcl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FvcqQOmOv1Jx"
      },
      "outputs": [],
      "source": [
        "def merge_data():\n",
        "  \n",
        "  #read data from file\n",
        "  df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/EmbeddingFileVocabData/Final_data_Y_D.csv')\n",
        "\n",
        "  #read data from file\n",
        "  new_df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data.csv', names=[\"Content\",\"Label\"])\n",
        "\n",
        "  #read data from file\n",
        "  add_df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data_new.csv', names=[\"Content\",\"Label\"])\n",
        "\n",
        "  #select only the required columns\n",
        "  df = df[[\"Content\",\"Label\"]]\n",
        "  \n",
        "  #concat two dataframes\n",
        "  final_df = pd.concat([df, new_df], ignore_index=True)\n",
        "\n",
        "  #concat two dataframes\n",
        "  new_gen_df = pd.concat([df, add_df], ignore_index=True)\n",
        "\n",
        "  process(df,\"YD_data\")\n",
        "  \n",
        "  process(final_df,\"YD_aug_data\")\n",
        "  \n",
        "  process(new_gen_df,\"YD_aug_data_balanced\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__:merge_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3TDgfT_XFfT",
        "outputId": "6d54b82e-d4a2-43bf-a0eb-0653bfeae077"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The length of dataset is: 440899\n",
            "\n",
            "Total number of values for classes is: 0    361594\n",
            "1     79305\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "The length of dataset is: 604529\n",
            "\n",
            "Total number of values for classes is: 0    361594\n",
            "1    242935\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "The length of dataset is: 726119\n",
            "\n",
            "Total number of values for classes is: 1    364525\n",
            "0    361594\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rVlLnMHZY2LM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Dataaugmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}