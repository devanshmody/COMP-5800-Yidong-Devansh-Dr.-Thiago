# -*- coding: utf-8 -*-
"""Dataaugmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13OipXmMy_3HHw-1RTs3bu3Kz7zbgVK8-
"""

!pip install transformers
!pip install numpy requests nlpaug
!pip install pandas==1.1.3

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import pandas as pd
import nlpaug
import nlpaug.augmenter.word as naw

#function to split dataframe into chuncks
def split_dataframe(df, chunk_size = 500):
  chunks = list()
  num_chunks = len(df) // chunk_size + 1
  for i in range(num_chunks):
      chunks.append(df[i*chunk_size:(i+1)*chunk_size])
  return chunks

  #function reads data collected and divides into chunks for data preprocessing
#this is a one time function call to divide into chunks
def divide_chunks():

  #read data 
  df = pd.read_csv("/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/EmbeddingFileVocabData/Final_data_Y_D.csv")

  df = df[df["Label"]!="label"]
  df = df[df["Label"]!="Label"]

  df.reset_index(drop=True,inplace=True)

  df["Label"] = df["Label"].apply(lambda x: int(x))

  #perform augmentation for only hate speech values as they are less
  df = df[df.Label.values==1]
  
  #rest index
  df.reset_index(drop = True, inplace=True)
  print("\nThe length of dataset is: {}".format(len(df)))


  #split dataframe into chuncks
  sp_df = split_dataframe(df)

  #store the chuncks into a seprate csv file
  for i in range(0,len(sp_df)):
    sp_df[i].to_csv("/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/augx{}.csv".format(i),index=False)

#divide_chunks()

#this function is used to perfrom data augmentation so we have uniformity in data
def aug_data(df):
  
  #perform a substitue augmentation
  aug = naw.ContextualWordEmbsAug(
    model_path='bert-base-uncased', action="substitute")

  #perform a insertion augmentation
  aug1 = naw.ContextualWordEmbsAug(
      model_path='bert-base-uncased', action="insert")

  aug2 = naw.SynonymAug(aug_src='wordnet', lang='eng')

  #perform augmentation for only hate speech values as they are less
  data = df[df.Label.values==1]

  aug_sentence = []
  lr=iter([item for item in range(0,len(df))])
  for value in data["Content"]:
    print("Processing for text {}".format(next(lr)))
    a1 = aug.augment(value,2) 
    for i in a1:
      aug_sentence.append(i)
    aug_sentence.append(aug1.augment(value,1))
    aug_sentence.append(aug2.augment(value))

  new_df = pd.DataFrame (aug_sentence, columns = ['Content'])
  new_df["Label"]=1

  #save augmented data 
  new_df.to_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data_new.csv',mode='a', index=False, header=False)

def main():
  #get the final cleaned data
  df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/augx155.csv')
  print("\nThe length of dataset is: {}".format(len(df)))

  aug_data(df)

if __name__:main()

#additional process applied on final data
def process(df,text):
  
  df = df[df["Label"]!="label"]
  df = df[df["Label"]!="Label"]

  df.reset_index(drop=True,inplace=True)

  df["Label"] = df["Label"].apply(lambda x: int(x))

  #rest index
  df.reset_index(drop = True, inplace=True)
  print("\nThe length of dataset is: {}".format(len(df)))

  print("\nTotal number of values for classes is: {}".format(df["Label"].value_counts()))

  #write data to the path
  df.to_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/FinalDataAndModel/{}.csv'.format(text),index=False)

def merge_data():
  
  #read data from file
  df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/EmbeddingFileVocabData/Final_data_Y_D.csv')

  #read data from file
  new_df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data.csv', names=["Content","Label"])

  #read data from file
  add_df = pd.read_csv('/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/Chunks/aug_data_new.csv', names=["Content","Label"])

  #select only the required columns
  df = df[["Content","Label"]]
  
  #concat two dataframes
  final_df = pd.concat([df, new_df], ignore_index=True)

  #concat two dataframes
  new_gen_df = pd.concat([df, add_df], ignore_index=True)

  process(df,"YD_data")
  
  process(final_df,"YD_aug_data")
  
  process(new_gen_df,"YD_aug_data_balanced")

if __name__:merge_data()

