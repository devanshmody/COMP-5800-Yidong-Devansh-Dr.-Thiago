# -*- coding: utf-8 -*-
"""DataVisualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gqOHg2kCEVKk9lEu3Hm2vnkrBmWpml6D
"""

import nltk
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px
import plotly.figure_factory as ff

#function to plot words 
def wc(data,bgcolor,title):
    plt.figure(figsize = (13,10))
    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)
    wc.generate(' '.join(data))
    plt.title(title , fontsize = 20)
    plt.imshow(wc)
    plt.axis('off')

#function to plot frequency of words in a bar style plot
def show_wfreq_plot(df):    
    freq_dist = nltk.FreqDist(df)    
    ls_freq = [(word, frequency) for word, frequency in freq_dist.most_common(20)]
    twdf = pd.DataFrame(ls_freq, columns=["Word", "Frequency"])
    tfig = px.bar(twdf, x="Word", y="Frequency", title="Top 20 most frequent words")
    tfig.show()

#function to plot some statistics about the data
def plot_some_stats(df,word_counts,missing_words,vocab_to_int):
  print("\nLength of the final data: {}".format(len(df)))
  print("\nSome information regarding data\n {}".format(df.info))
  print("\nDescription of data\n {}".format(df.describe()))
  print("\nTotal count of labels\n {}".format(df["Label"].value_counts()))
  print("\nSamples which are not Hate Speech\n {}".format(df.loc[df.Label == str(0), ['Content']].sample(5).values))
  print("\nSamples which are Hate Speech\n {}".format(df.loc[df.Label == str(1), ['Content']].sample(5).values))
  #find ratio of missing words
  missing_ratio = round(len(missing_words)/len(word_counts),4)*100
  print("\nNumber of words missing from word_embeddings:", len(missing_words))
  print("\nPercent of words that are missing from our vocabulary: {}%".format(missing_ratio))
  print("\nvoc_to_int_ index position of tokens UNK is {}, PAD is {}, EOS is {} and GO is {} ".format(vocab_to_int['<UNK>'],vocab_to_int['<PAD>'],vocab_to_int['<EOS>'],vocab_to_int['<GO>']))
  print("\n",df['Label'].value_counts().plot(kind = 'pie'))
  print("\n",wc(word_counts,"black","Most Frequent Words"))
  print("\n",show_wfreq_plot(word_counts))
  print("\n",wc(missing_words,"black","Missing Words"))

#function to plot the length of training, validation and testing
def plot_tr_tval_tt_len(xtr,xval,xtt):
  names = ['Training','Validation','Testing']
  values = [len(xtr),len(xval),len(xtt)]
  plt.figure(figsize=(10,5))
  plt.subplot(131)
  plt.bar(names,values,color=['darkorange','coral','coral'],edgecolor='darkblue')
  plt.suptitle('Categorical Plotting')
  plt.show()

#function to plot loss and accuracy curves on training and validation set
def plotgraph(history):
  plt.figure(figsize=[8,6])
  plt.plot(history.history['loss'],'firebrick',linewidth=3.0)
  plt.plot(history.history['accuracy'],'turquoise',linewidth=3.0)
  plt.plot(history.history['val_loss'],'midnightblue',linewidth=3.0)
  plt.legend(['Training loss','Training Accuracy','Validation loss'],fontsize=18)
  plt.xlabel('Epochs',fontsize=16)
  plt.ylabel('Loss and Accuracy',fontsize=16)
  plt.title('Loss Curves and Accuracy Curves for text summarization',fontsize=16)