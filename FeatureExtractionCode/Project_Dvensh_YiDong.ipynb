{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_Dvensh_YiDong.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"oyDu8oQMDRJs"},"source":["pip install chart-studio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adzc-kW3HygY","outputId":"df2c7225-5af5-419e-923b-3f499c41f934"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"gkyC4WCEIdPV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7852268-94a8-494a-ac34-e84e507379cb"},"source":["#Character Embedding\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from keras.models import Model\n","from keras.layers import Dense, Embedding, Input\n","from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n","from keras.preprocessing import text as keras_text, sequence as keras_seq\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","max_features = 64\n","maxlen = 512\n","#read our files\n","train = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDataSet/data_huang.csv\")\n","test = pd.read_csv(\"/content/drive/MyDrive/HateSpeechDataSet/data_huang.csv\")\n","train = train.sample(frac=1)\n","\n","list_sentences_train = train[\"Content\"].fillna(\"unknown\").values\n","list_classes = [\"Label\"]\n","y = train[list_classes].values\n","list_sentences_test = test[\"Content\"].fillna(\"unknown\").values\n","\n","#Sequence generation\n","tokenizer = keras_text.Tokenizer(char_level = True)\n","tokenizer.fit_on_texts(list(list_sentences_train))\n","# train data\n","list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n","X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n","# test data\n","list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n","X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0 ...  6 15  7]\n"," [ 0  0  0 ... 12  9 11]\n"," [ 0  0  0 ...  8  7  1]\n"," ...\n"," [ 0  0  0 ... 42  1  1]\n"," [ 0  0  0 ... 13  4 14]\n"," [ 0  0  0 ... 25  1  1]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3mCnpRdViD-","outputId":"c2d5b05e-4af0-4ee2-eacb-a7028acd7efe"},"source":["#model test\n","def build_model(conv_layers = 2, \n","                dilation_rates = [0, 2, 4, 8, 16], \n","                embed_size = 256):\n","    inp = Input(shape=(None, ))\n","    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n","                  output_dim = embed_size)(inp)\n","    prefilt_x = Dropout(0.25)(x)\n","    out_conv = []\n","    # dilation rate lets us use ngrams and skip grams to process \n","    for dilation_rate in dilation_rates:\n","        x = prefilt_x\n","        for i in range(2):\n","            if dilation_rate>0:\n","                x = Conv1D(16*2**(i), \n","                           kernel_size = 3, \n","                           dilation_rate = dilation_rate,\n","                          activation = 'relu',\n","                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n","                          )(x)\n","            else:\n","                x = Conv1D(16*2**(i), \n","                           kernel_size = 1,\n","                          activation = 'relu',\n","                          name = 'word_fcl_{}'.format(i))(x)\n","        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n","    x = concatenate(out_conv, axis = -1)    \n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(0.1)(x)\n","    x = Dense(32, activation='relu')(x)\n","    x = Dropout(0.1)(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","    model = Model(inputs=inp, outputs=x)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    return model\n","\n","model = build_model()\n","model.summary()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 256)    52480       ['input_2[0][0]']                \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, None, 256)    0           ['embedding_1[0][0]']            \n","                                                                                                  \n"," word_fcl_0 (Conv1D)            (None, None, 16)     4112        ['dropout_8[0][0]']              \n","                                                                                                  \n"," ngram_2_cnn_0 (Conv1D)         (None, None, 16)     12304       ['dropout_8[0][0]']              \n","                                                                                                  \n"," ngram_4_cnn_0 (Conv1D)         (None, None, 16)     12304       ['dropout_8[0][0]']              \n","                                                                                                  \n"," ngram_8_cnn_0 (Conv1D)         (None, None, 16)     12304       ['dropout_8[0][0]']              \n","                                                                                                  \n"," ngram_16_cnn_0 (Conv1D)        (None, None, 16)     12304       ['dropout_8[0][0]']              \n","                                                                                                  \n"," word_fcl_1 (Conv1D)            (None, None, 32)     544         ['word_fcl_0[0][0]']             \n","                                                                                                  \n"," ngram_2_cnn_1 (Conv1D)         (None, None, 32)     1568        ['ngram_2_cnn_0[0][0]']          \n","                                                                                                  \n"," ngram_4_cnn_1 (Conv1D)         (None, None, 32)     1568        ['ngram_4_cnn_0[0][0]']          \n","                                                                                                  \n"," ngram_8_cnn_1 (Conv1D)         (None, None, 32)     1568        ['ngram_8_cnn_0[0][0]']          \n","                                                                                                  \n"," ngram_16_cnn_1 (Conv1D)        (None, None, 32)     1568        ['ngram_16_cnn_0[0][0]']         \n","                                                                                                  \n"," global_max_pooling1d_5 (Global  (None, 32)          0           ['word_fcl_1[0][0]']             \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," global_max_pooling1d_6 (Global  (None, 32)          0           ['ngram_2_cnn_1[0][0]']          \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," global_max_pooling1d_7 (Global  (None, 32)          0           ['ngram_4_cnn_1[0][0]']          \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," global_max_pooling1d_8 (Global  (None, 32)          0           ['ngram_8_cnn_1[0][0]']          \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," global_max_pooling1d_9 (Global  (None, 32)          0           ['ngram_16_cnn_1[0][0]']         \n"," MaxPooling1D)                                                                                    \n","                                                                                                  \n"," dropout_9 (Dropout)            (None, 32)           0           ['global_max_pooling1d_5[0][0]'] \n","                                                                                                  \n"," dropout_10 (Dropout)           (None, 32)           0           ['global_max_pooling1d_6[0][0]'] \n","                                                                                                  \n"," dropout_11 (Dropout)           (None, 32)           0           ['global_max_pooling1d_7[0][0]'] \n","                                                                                                  \n"," dropout_12 (Dropout)           (None, 32)           0           ['global_max_pooling1d_8[0][0]'] \n","                                                                                                  \n"," dropout_13 (Dropout)           (None, 32)           0           ['global_max_pooling1d_9[0][0]'] \n","                                                                                                  \n"," concatenate_1 (Concatenate)    (None, 160)          0           ['dropout_9[0][0]',              \n","                                                                  'dropout_10[0][0]',             \n","                                                                  'dropout_11[0][0]',             \n","                                                                  'dropout_12[0][0]',             \n","                                                                  'dropout_13[0][0]']             \n","                                                                                                  \n"," dense_3 (Dense)                (None, 64)           10304       ['concatenate_1[0][0]']          \n","                                                                                                  \n"," dropout_14 (Dropout)           (None, 64)           0           ['dense_3[0][0]']                \n","                                                                                                  \n"," dense_4 (Dense)                (None, 32)           2080        ['dropout_14[0][0]']             \n","                                                                                                  \n"," dropout_15 (Dropout)           (None, 32)           0           ['dense_4[0][0]']                \n","                                                                                                  \n"," dense_5 (Dense)                (None, 1)            33          ['dropout_15[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 125,041\n","Trainable params: 125,041\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Qfr0VEgtVpKi","outputId":"56b1df52-adb8-4623-d229-ac8bac776630"},"source":["# Train and Test\n","from sklearn.model_selection import train_test_split\n","any_category_positive = np.sum(y,1)\n","print('Distribution of Total Positive Labels (important for validation)')\n","print(pd.value_counts(any_category_positive))\n","X_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y, \n","                                                        test_size = 0.2, \n","                                                        stratify = any_category_positive,\n","                                                       random_state = 2017)\n","print('Training:', X_t_train.shape)\n","print('Testing:', X_t_test.shape)\n","\n","batch_size = 128 # large enough that some other labels come in\n","epochs = 1\n","\n","file_path=\"best_weights.h5\"\n","checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n","\n","callbacks_list = [checkpoint, early] #early\n","model.fit(X_t_train, y_train, \n","          validation_data=(X_t_test, y_test),\n","          batch_size=batch_size, \n","          epochs=epochs, \n","          shuffle = True,\n","          callbacks=callbacks_list)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Distribution of Total Positive Labels (important for validation)\n","0    43111\n","1    11819\n","dtype: int64\n","Training: (43944, 512)\n","Testing: (10986, 512)\n","344/344 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8140\n","Epoch 00001: val_loss improved from inf to 0.35762, saving model to best_weights.h5\n","344/344 [==============================] - 633s 2s/step - loss: 0.4297 - accuracy: 0.8140 - val_loss: 0.3576 - val_accuracy: 0.8550\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f7320849850>"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}]}