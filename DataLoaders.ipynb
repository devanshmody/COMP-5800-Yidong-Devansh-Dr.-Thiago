{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A Curated Hate Speech Dataset</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Follwoing are the steps to download and use the data</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1)</b> Download the data from the following given link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/9sxpkmm8xn-1.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2)</b> Unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip files\n",
    "!unzip /Users/devanshmody/GithubDataLoaders/9sxpkmm8xn-1.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3)</b> Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4)</b> Fetch the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/0_RawData/data_huang_devansh.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 5)</b> Load the contractions and profanities dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load contractions\n",
    "def get_contractions(path):\n",
    "    \n",
    "    #import the dictionary of single contractions \n",
    "    with open(path,'r') as f:\n",
    "        data = f.read()\n",
    "         \n",
    "    #reconstructing the data as a dictionary\n",
    "    contractions = ast.literal_eval(data)\n",
    "\n",
    "    contractions = {k.strip():v.strip() for k,v in contractions.items()}\n",
    "\n",
    "    return contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load addon contractions\n",
    "addon_contractions = get_contractions(\"A Curated Hate Speech Dataset/HSData/1_ContractionProfanitiesEnglish/AddonDoubleContractions\")\n",
    "\n",
    "#load double contractions\n",
    "double_contractions = get_contractions(\"A Curated Hate Speech Dataset/HSData/1_ContractionProfanitiesEnglish/DoubleContractions\")\n",
    "\n",
    "#load single contractions\n",
    "single_contractions = get_contractions(\"A Curated Hate Speech Dataset/HSData/1_ContractionProfanitiesEnglish/Contractions\")\n",
    "\n",
    "#load profanities dataset\n",
    "profanities = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/1_ContractionProfanitiesEnglish/Profanities.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 6)</b> Load the final dataset used for training and augmented dataset which is used to generate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load final dataset \n",
    "final_dataset = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/3_DataAugmentationAndBERTVocab/Final_data_Y_D.csv\")\n",
    "\n",
    "#load augmented dataset\n",
    "augmented_dataset = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/3_DataAugmentationAndBERTVocab/YD_aug_data_balanced.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 7)</b> Load the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/COMP-5800-YDE-Yidong-Devansh-Final-Project/pretrained-bert\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 8)</b> Load the training and validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train folds\n",
    "train_fold = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/TrainFolds/train_kfoldx0-3\")\n",
    "\n",
    "#load validation folds\n",
    "validation_fold0 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/ValidationFolds/validation_kfoldx0\")\n",
    "validation_fold1 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/ValidationFolds/validation_kfoldx1\")\n",
    "validation_fold2 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/ValidationFolds/validation_kfoldx2\")\n",
    "validation_fold3 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/ValidationFolds/validation_kfoldx3\")\n",
    "validation_fold4 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/5_TrainingValidationFolds/ValidationFolds/validation_kfoldx4\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 9)</b> Load the missclassified folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the missclassified folds\n",
    "missclassifed_fold0 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/6_MissclassifiedBERT2DataFolds/Y_D_missclassified_kfold0.csv\")\n",
    "missclassifed_fold1 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/6_MissclassifiedBERT2DataFolds/Y_D_missclassified_kfold1.csv\")\n",
    "missclassifed_fold2 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/6_MissclassifiedBERT2DataFolds/Y_D_missclassified_kfold2.csv\")\n",
    "missclassifed_fold3 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/6_MissclassifiedBERT2DataFolds/Y_D_missclassified_kfold3.csv\")\n",
    "missclassifed_fold4 = pd.read_csv(\"A Curated Hate Speech Dataset/HSData/6_MissclassifiedBERT2DataFolds/Y_D_missclassified_kfold_4.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 10)</b> Load the vectors for BILSTM-Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vectors for tokenization of input\n",
    "from_disk = pickle.load(open(\"A Curated Hate Speech Dataset/HSData/7_BILSTM-Model1/tv_layer.pkl\", \"rb\"))\n",
    "\n",
    "#configure the keras layer with loaded vectors\n",
    "vectorizer = tf.keras.layers.TextVectorization.from_config(from_disk['config'])\n",
    "  \n",
    "#adapt has to be called with some dummy data (BUG in Keras)\n",
    "vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "\n",
    "#set the weights of the vectorizer\n",
    "vectorizer.set_weights(from_disk['weights'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 11)</b> Load the integer to vocabulary, vocabulary to integer and word embeddings for BISLTM2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dictionary of index to vocab mapping\n",
    "with open('A Curated Hate Speech Dataset/HSData/8_BILSTM-Model2/int_to_vocab.txt','r') as f:\n",
    "  data = f.read()\n",
    "        \n",
    "#reconstructing the data as a dictionary\n",
    "int_to_vocab = ast.literal_eval(data)\n",
    "\n",
    "#import the dictionary of vocab to index mapping\n",
    "with open('A Curated Hate Speech Dataset/HSData/8_BILSTM-Model2/vocab_to_int.txt','r') as f:\n",
    "  data = f.read()\n",
    "        \n",
    "#reconstructing the data as a dictionary\n",
    "vocab_to_int = ast.literal_eval(data)\n",
    "\n",
    "#convert into the required format\n",
    "int_to_vocab = {int(k):str(v) for k,v in int_to_vocab.items()}\n",
    "vocab_to_int = {str(k):int(v) for k,v in vocab_to_int.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 12)</b> Load the TFIDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open(\"A Curated Hate Speech Dataset/HSData/9_TFIDF/tfidf.pkl\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 13)</b> Load the gabb and reddit datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gab_train = \"A Curated Hate Speech Dataset/HSData/10_GabbAndRedditComparison/gabb_reddit_train.csv\"\n",
    "gabb_test = \"A Curated Hate Speech Dataset/HSData/10_GabbAndRedditComparison/gabb_reddit_test.csv\"\n",
    "missclassified = \"A Curated Hate Speech Dataset/HSData/10_GabbAndRedditComparison/Y_D_incorrect.csv\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98813eb70776a6b7862b59a5cd6ded26e58148f6ad56fa9d16a2a8186cf6acab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
