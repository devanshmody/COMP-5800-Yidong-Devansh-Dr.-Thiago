# -*- coding: utf-8 -*-
"""FirstBert_PieChart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WHniV5uBzCk5zZ_IeAXEvPLKl3OwrpSL
"""

pip install chart-studio

import numpy as np 
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

chart_data = pd.read_csv("/content/drive/MyDrive/HateSpeechDataSet/data_huang_devansh.csv")
df_chart_data_1 = chart_data[chart_data['Label']==1]
df_chart_data_0 = chart_data[chart_data['Label']==0]
print(df_chart_data_1)
print(df_chart_data_0)

df_500 =  chart_data.loc[chart_data['Content'].str.len() < 500]

import matplotlib.pyplot as plt

labels = 'HateSpeech', 'Normol word'
# we need to change the value at here
sizes = [133694, 708641]

# configure the distand between two parts in pie chart
explode = (0, 0.1) 

plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)

# Equal aspect ratio 
plt.axis('equal') 

plt.show()

import matplotlib.pyplot as plt

list_length = []
list_length_1000 = []
list_length_500 = []
sentence = chart_data['Content']
#842334
for i in range(0,1000):
  sentence_temp = sentence[i]
  word_list = sentence_temp.split(' ')
  len_temp = len(word_list)
  if(len_temp > 500):
    list_length_1000.append(len_temp)
  else:
    list_length_500.append(len_temp) 

#plt.bar(range(len(list_length)),list_length)

#plt.show()

labels = 'long scentence above 500 words', 'Normol scentenc below 500 words'
sizes = [len(list_length_1000), len(list_length_500)]

# configure the distand between two parts in pie chart
explode = (0, 0.1) 

plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)

# Equal aspect ratio 
plt.axis('equal') 

plt.show()

!pip install transformers
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings
warnings.filterwarnings('ignore')

#df = pd.read_csv('/content/drive/MyDrive/HateSpeechDataSet/data_huang_devansh.csv')
#df_500 =  chart_data.loc[chart_data['Content'].str.len() < 500]
df= df_500
batch_1 = df[:100]


# For DistilBERT:
model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')

# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

tokenized = batch_1['Content'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))

#Padding
max_len = 0
for i in tokenized.values:
    if len(i) > max_len:
        max_len = len(i)

padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])
attention_mask = np.where(padded != 0, 1, 0)
attention_mask.shape

#model The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.
input_ids = torch.tensor(padded)  
attention_mask = torch.tensor(attention_mask)

with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)

features = last_hidden_states[0][:,0,:].numpy()
labels = batch_1['Label']
print(features) #t